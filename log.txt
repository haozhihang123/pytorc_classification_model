I0117 19:16:26.088300 20908 caffe.cpp:218] Using GPUs 0
I0117 19:16:26.195992 20908 caffe.cpp:223] GPU 0: GeForce GTX 1050
I0117 19:16:26.528070 20908 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I0117 19:16:26.528070 20908 solver.cpp:48] Initializing solver from parameters: 
test_iter: 20
test_interval: 1000
base_lr: 0.01
display: 1000
max_iter: 20000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "H:/1/caffe-windows/examples/face/model/"
solver_mode: GPU
device_id: 0
net: "H:/1/caffe-windows/examples/face/lenet_train_test.prototxt"
train_state {
  level: 0
  stage: ""
}
I0117 19:16:26.530064 20908 solver.cpp:91] Creating training net from net file: H:/1/caffe-windows/examples/face/lenet_train_test.prototxt
I0117 19:16:26.530064 20908 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0117 19:16:26.530064 20908 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0117 19:16:26.530064 20908 net.cpp:58] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "H:/1/caffe-windows/examples/face/lmdb/train"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 7
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0117 19:16:26.530064 20908 layer_factory.cpp:58] Creating layer mnist
I0117 19:16:26.530064 20908 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I0117 19:16:26.531061 20908 net.cpp:100] Creating Layer mnist
I0117 19:16:26.531061 20908 net.cpp:408] mnist -> data
I0117 19:16:26.531061 20908 net.cpp:408] mnist -> label
I0117 19:16:26.531061 29944 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I0117 19:16:26.533082 29944 db_lmdb.cpp:40] Opened lmdb H:/1/caffe-windows/examples/face/lmdb/train
I0117 19:16:26.595890 20908 data_layer.cpp:41] output data size: 64,1,48,48
I0117 19:16:26.596886 20908 net.cpp:150] Setting up mnist
I0117 19:16:26.596886 20908 net.cpp:157] Top shape: 64 1 48 48 (147456)
I0117 19:16:26.596886 20908 net.cpp:157] Top shape: 64 (64)
I0117 19:16:26.596886 20908 net.cpp:165] Memory required for data: 590080
I0117 19:16:26.596886 20908 layer_factory.cpp:58] Creating layer conv1
I0117 19:16:26.596886 20908 net.cpp:100] Creating Layer conv1
I0117 19:16:26.596886 20908 net.cpp:434] conv1 <- data
I0117 19:16:26.596886 20908 net.cpp:408] conv1 -> conv1
I0117 19:16:26.597908 19492 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I0117 19:16:27.569285 20908 net.cpp:150] Setting up conv1
I0117 19:16:27.569285 20908 net.cpp:157] Top shape: 64 20 44 44 (2478080)
I0117 19:16:27.569285 20908 net.cpp:165] Memory required for data: 10502400
I0117 19:16:27.569285 20908 layer_factory.cpp:58] Creating layer pool1
I0117 19:16:27.569285 20908 net.cpp:100] Creating Layer pool1
I0117 19:16:27.569285 20908 net.cpp:434] pool1 <- conv1
I0117 19:16:27.569285 20908 net.cpp:408] pool1 -> pool1
I0117 19:16:27.569285 20908 net.cpp:150] Setting up pool1
I0117 19:16:27.569285 20908 net.cpp:157] Top shape: 64 20 22 22 (619520)
I0117 19:16:27.569285 20908 net.cpp:165] Memory required for data: 12980480
I0117 19:16:27.569285 20908 layer_factory.cpp:58] Creating layer conv2
I0117 19:16:27.569285 20908 net.cpp:100] Creating Layer conv2
I0117 19:16:27.569285 20908 net.cpp:434] conv2 <- pool1
I0117 19:16:27.569285 20908 net.cpp:408] conv2 -> conv2
I0117 19:16:27.572276 20908 net.cpp:150] Setting up conv2
I0117 19:16:27.572276 20908 net.cpp:157] Top shape: 64 50 18 18 (1036800)
I0117 19:16:27.572276 20908 net.cpp:165] Memory required for data: 17127680
I0117 19:16:27.572276 20908 layer_factory.cpp:58] Creating layer pool2
I0117 19:16:27.572276 20908 net.cpp:100] Creating Layer pool2
I0117 19:16:27.572276 20908 net.cpp:434] pool2 <- conv2
I0117 19:16:27.572276 20908 net.cpp:408] pool2 -> pool2
I0117 19:16:27.572276 20908 net.cpp:150] Setting up pool2
I0117 19:16:27.572276 20908 net.cpp:157] Top shape: 64 50 9 9 (259200)
I0117 19:16:27.572276 20908 net.cpp:165] Memory required for data: 18164480
I0117 19:16:27.572276 20908 layer_factory.cpp:58] Creating layer ip1
I0117 19:16:27.572276 20908 net.cpp:100] Creating Layer ip1
I0117 19:16:27.572276 20908 net.cpp:434] ip1 <- pool2
I0117 19:16:27.572276 20908 net.cpp:408] ip1 -> ip1
I0117 19:16:27.584275 20908 net.cpp:150] Setting up ip1
I0117 19:16:27.584275 20908 net.cpp:157] Top shape: 64 500 (32000)
I0117 19:16:27.584275 20908 net.cpp:165] Memory required for data: 18292480
I0117 19:16:27.584275 20908 layer_factory.cpp:58] Creating layer relu1
I0117 19:16:27.584275 20908 net.cpp:100] Creating Layer relu1
I0117 19:16:27.584275 20908 net.cpp:434] relu1 <- ip1
I0117 19:16:27.584275 20908 net.cpp:395] relu1 -> ip1 (in-place)
I0117 19:16:27.585242 20908 net.cpp:150] Setting up relu1
I0117 19:16:27.585242 20908 net.cpp:157] Top shape: 64 500 (32000)
I0117 19:16:27.585242 20908 net.cpp:165] Memory required for data: 18420480
I0117 19:16:27.585242 20908 layer_factory.cpp:58] Creating layer ip2
I0117 19:16:27.585242 20908 net.cpp:100] Creating Layer ip2
I0117 19:16:27.585242 20908 net.cpp:434] ip2 <- ip1
I0117 19:16:27.585242 20908 net.cpp:408] ip2 -> ip2
I0117 19:16:27.585242 20908 net.cpp:150] Setting up ip2
I0117 19:16:27.586273 20908 net.cpp:157] Top shape: 64 7 (448)
I0117 19:16:27.586273 20908 net.cpp:165] Memory required for data: 18422272
I0117 19:16:27.586273 20908 layer_factory.cpp:58] Creating layer loss
I0117 19:16:27.586273 20908 net.cpp:100] Creating Layer loss
I0117 19:16:27.586273 20908 net.cpp:434] loss <- ip2
I0117 19:16:27.586273 20908 net.cpp:434] loss <- label
I0117 19:16:27.586273 20908 net.cpp:408] loss -> loss
I0117 19:16:27.586273 20908 layer_factory.cpp:58] Creating layer loss
I0117 19:16:27.586273 20908 net.cpp:150] Setting up loss
I0117 19:16:27.586273 20908 net.cpp:157] Top shape: (1)
I0117 19:16:27.586273 20908 net.cpp:160]     with loss weight 1
I0117 19:16:27.586273 20908 net.cpp:165] Memory required for data: 18422276
I0117 19:16:27.586273 20908 net.cpp:226] loss needs backward computation.
I0117 19:16:27.586273 20908 net.cpp:226] ip2 needs backward computation.
I0117 19:16:27.586273 20908 net.cpp:226] relu1 needs backward computation.
I0117 19:16:27.586273 20908 net.cpp:226] ip1 needs backward computation.
I0117 19:16:27.586273 20908 net.cpp:226] pool2 needs backward computation.
I0117 19:16:27.586273 20908 net.cpp:226] conv2 needs backward computation.
I0117 19:16:27.586273 20908 net.cpp:226] pool1 needs backward computation.
I0117 19:16:27.586273 20908 net.cpp:226] conv1 needs backward computation.
I0117 19:16:27.586273 20908 net.cpp:228] mnist does not need backward computation.
I0117 19:16:27.586273 20908 net.cpp:270] This network produces output loss
I0117 19:16:27.586273 20908 net.cpp:283] Network initialization done.
I0117 19:16:27.587267 20908 solver.cpp:181] Creating test net (#0) specified by net file: H:/1/caffe-windows/examples/face/lenet_train_test.prototxt
I0117 19:16:27.587267 20908 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0117 19:16:27.587267 20908 net.cpp:58] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "H:/1/caffe-windows/examples/face/lmdb/test"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 7
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0117 19:16:27.587267 20908 layer_factory.cpp:58] Creating layer mnist
I0117 19:16:27.587267 20908 net.cpp:100] Creating Layer mnist
I0117 19:16:27.587267 20908 net.cpp:408] mnist -> data
I0117 19:16:27.587267 20908 net.cpp:408] mnist -> label
I0117 19:16:27.588268 23248 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I0117 19:16:27.589262 23248 db_lmdb.cpp:40] Opened lmdb H:/1/caffe-windows/examples/face/lmdb/test
I0117 19:16:27.590253 20908 data_layer.cpp:41] output data size: 100,1,48,48
I0117 19:16:27.593221 20908 net.cpp:150] Setting up mnist
I0117 19:16:27.593221 20908 net.cpp:157] Top shape: 100 1 48 48 (230400)
I0117 19:16:27.593221 20908 net.cpp:157] Top shape: 100 (100)
I0117 19:16:27.593221 20908 net.cpp:165] Memory required for data: 922000
I0117 19:16:27.593221 20908 layer_factory.cpp:58] Creating layer label_mnist_1_split
I0117 19:16:27.593221 20908 net.cpp:100] Creating Layer label_mnist_1_split
I0117 19:16:27.593221 20908 net.cpp:434] label_mnist_1_split <- label
I0117 19:16:27.593221 20908 net.cpp:408] label_mnist_1_split -> label_mnist_1_split_0
I0117 19:16:27.593221 20908 net.cpp:408] label_mnist_1_split -> label_mnist_1_split_1
I0117 19:16:27.594218 20908 net.cpp:150] Setting up label_mnist_1_split
I0117 19:16:27.594218 20908 net.cpp:157] Top shape: 100 (100)
I0117 19:16:27.594218 20908 net.cpp:157] Top shape: 100 (100)
I0117 19:16:27.594218 20908 net.cpp:165] Memory required for data: 922800
I0117 19:16:27.594218 20908 layer_factory.cpp:58] Creating layer conv1
I0117 19:16:27.594218 20908 net.cpp:100] Creating Layer conv1
I0117 19:16:27.594218 20908 net.cpp:434] conv1 <- data
I0117 19:16:27.594218 20908 net.cpp:408] conv1 -> conv1
I0117 19:16:27.595239 27420 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I0117 19:16:27.597209 20908 net.cpp:150] Setting up conv1
I0117 19:16:27.597209 20908 net.cpp:157] Top shape: 100 20 44 44 (3872000)
I0117 19:16:27.597209 20908 net.cpp:165] Memory required for data: 16410800
I0117 19:16:27.597209 20908 layer_factory.cpp:58] Creating layer pool1
I0117 19:16:27.597209 20908 net.cpp:100] Creating Layer pool1
I0117 19:16:27.597209 20908 net.cpp:434] pool1 <- conv1
I0117 19:16:27.597209 20908 net.cpp:408] pool1 -> pool1
I0117 19:16:27.597209 20908 net.cpp:150] Setting up pool1
I0117 19:16:27.597209 20908 net.cpp:157] Top shape: 100 20 22 22 (968000)
I0117 19:16:27.597209 20908 net.cpp:165] Memory required for data: 20282800
I0117 19:16:27.597209 20908 layer_factory.cpp:58] Creating layer conv2
I0117 19:16:27.597209 20908 net.cpp:100] Creating Layer conv2
I0117 19:16:27.597209 20908 net.cpp:434] conv2 <- pool1
I0117 19:16:27.597209 20908 net.cpp:408] conv2 -> conv2
I0117 19:16:27.599203 20908 net.cpp:150] Setting up conv2
I0117 19:16:27.599203 20908 net.cpp:157] Top shape: 100 50 18 18 (1620000)
I0117 19:16:27.599203 20908 net.cpp:165] Memory required for data: 26762800
I0117 19:16:27.599203 20908 layer_factory.cpp:58] Creating layer pool2
I0117 19:16:27.599203 20908 net.cpp:100] Creating Layer pool2
I0117 19:16:27.599203 20908 net.cpp:434] pool2 <- conv2
I0117 19:16:27.599203 20908 net.cpp:408] pool2 -> pool2
I0117 19:16:27.599203 20908 net.cpp:150] Setting up pool2
I0117 19:16:27.599203 20908 net.cpp:157] Top shape: 100 50 9 9 (405000)
I0117 19:16:27.599203 20908 net.cpp:165] Memory required for data: 28382800
I0117 19:16:27.599203 20908 layer_factory.cpp:58] Creating layer ip1
I0117 19:16:27.599203 20908 net.cpp:100] Creating Layer ip1
I0117 19:16:27.599203 20908 net.cpp:434] ip1 <- pool2
I0117 19:16:27.599203 20908 net.cpp:408] ip1 -> ip1
I0117 19:16:27.612203 20908 net.cpp:150] Setting up ip1
I0117 19:16:27.612203 20908 net.cpp:157] Top shape: 100 500 (50000)
I0117 19:16:27.612203 20908 net.cpp:165] Memory required for data: 28582800
I0117 19:16:27.612203 20908 layer_factory.cpp:58] Creating layer relu1
I0117 19:16:27.612203 20908 net.cpp:100] Creating Layer relu1
I0117 19:16:27.612203 20908 net.cpp:434] relu1 <- ip1
I0117 19:16:27.612203 20908 net.cpp:395] relu1 -> ip1 (in-place)
I0117 19:16:27.612203 20908 net.cpp:150] Setting up relu1
I0117 19:16:27.612203 20908 net.cpp:157] Top shape: 100 500 (50000)
I0117 19:16:27.612203 20908 net.cpp:165] Memory required for data: 28782800
I0117 19:16:27.612203 20908 layer_factory.cpp:58] Creating layer ip2
I0117 19:16:27.612203 20908 net.cpp:100] Creating Layer ip2
I0117 19:16:27.612203 20908 net.cpp:434] ip2 <- ip1
I0117 19:16:27.612203 20908 net.cpp:408] ip2 -> ip2
I0117 19:16:27.613198 20908 net.cpp:150] Setting up ip2
I0117 19:16:27.613198 20908 net.cpp:157] Top shape: 100 7 (700)
I0117 19:16:27.613198 20908 net.cpp:165] Memory required for data: 28785600
I0117 19:16:27.613198 20908 layer_factory.cpp:58] Creating layer ip2_ip2_0_split
I0117 19:16:27.613198 20908 net.cpp:100] Creating Layer ip2_ip2_0_split
I0117 19:16:27.613198 20908 net.cpp:434] ip2_ip2_0_split <- ip2
I0117 19:16:27.613198 20908 net.cpp:408] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0117 19:16:27.613198 20908 net.cpp:408] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0117 19:16:27.613198 20908 net.cpp:150] Setting up ip2_ip2_0_split
I0117 19:16:27.613198 20908 net.cpp:157] Top shape: 100 7 (700)
I0117 19:16:27.613198 20908 net.cpp:157] Top shape: 100 7 (700)
I0117 19:16:27.613198 20908 net.cpp:165] Memory required for data: 28791200
I0117 19:16:27.613198 20908 layer_factory.cpp:58] Creating layer accuracy
I0117 19:16:27.613198 20908 net.cpp:100] Creating Layer accuracy
I0117 19:16:27.613198 20908 net.cpp:434] accuracy <- ip2_ip2_0_split_0
I0117 19:16:27.613198 20908 net.cpp:434] accuracy <- label_mnist_1_split_0
I0117 19:16:27.613198 20908 net.cpp:408] accuracy -> accuracy
I0117 19:16:27.613198 20908 net.cpp:150] Setting up accuracy
I0117 19:16:27.613198 20908 net.cpp:157] Top shape: (1)
I0117 19:16:27.613198 20908 net.cpp:165] Memory required for data: 28791204
I0117 19:16:27.613198 20908 layer_factory.cpp:58] Creating layer loss
I0117 19:16:27.613198 20908 net.cpp:100] Creating Layer loss
I0117 19:16:27.613198 20908 net.cpp:434] loss <- ip2_ip2_0_split_1
I0117 19:16:27.613198 20908 net.cpp:434] loss <- label_mnist_1_split_1
I0117 19:16:27.613198 20908 net.cpp:408] loss -> loss
I0117 19:16:27.613198 20908 layer_factory.cpp:58] Creating layer loss
I0117 19:16:27.614164 20908 net.cpp:150] Setting up loss
I0117 19:16:27.614164 20908 net.cpp:157] Top shape: (1)
I0117 19:16:27.614164 20908 net.cpp:160]     with loss weight 1
I0117 19:16:27.614164 20908 net.cpp:165] Memory required for data: 28791208
I0117 19:16:27.614164 20908 net.cpp:226] loss needs backward computation.
I0117 19:16:27.614164 20908 net.cpp:228] accuracy does not need backward computation.
I0117 19:16:27.614164 20908 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0117 19:16:27.614164 20908 net.cpp:226] ip2 needs backward computation.
I0117 19:16:27.614164 20908 net.cpp:226] relu1 needs backward computation.
I0117 19:16:27.614164 20908 net.cpp:226] ip1 needs backward computation.
I0117 19:16:27.614164 20908 net.cpp:226] pool2 needs backward computation.
I0117 19:16:27.614164 20908 net.cpp:226] conv2 needs backward computation.
I0117 19:16:27.614164 20908 net.cpp:226] pool1 needs backward computation.
I0117 19:16:27.614164 20908 net.cpp:226] conv1 needs backward computation.
I0117 19:16:27.614164 20908 net.cpp:228] label_mnist_1_split does not need backward computation.
I0117 19:16:27.614164 20908 net.cpp:228] mnist does not need backward computation.
I0117 19:16:27.614164 20908 net.cpp:270] This network produces output accuracy
I0117 19:16:27.614164 20908 net.cpp:270] This network produces output loss
I0117 19:16:27.614164 20908 net.cpp:283] Network initialization done.
I0117 19:16:27.614164 20908 solver.cpp:60] Solver scaffolding done.
I0117 19:16:27.614164 20908 caffe.cpp:252] Starting Optimization
I0117 19:16:27.614164 20908 solver.cpp:279] Solving LeNet
I0117 19:16:27.614164 20908 solver.cpp:280] Learning Rate Policy: inv
I0117 19:16:27.615162 20908 solver.cpp:337] Iteration 0, Testing net (#0)
I0117 19:16:27.732847 20908 solver.cpp:404]     Test net output #0: accuracy = 0.142
I0117 19:16:27.732847 20908 solver.cpp:404]     Test net output #1: loss = 2.00654 (* 1 = 2.00654 loss)
I0117 19:16:27.741822 20908 solver.cpp:228] Iteration 0, loss = 2.0125
I0117 19:16:27.741822 20908 solver.cpp:244]     Train net output #0: loss = 2.0125 (* 1 = 2.0125 loss)
I0117 19:16:27.741822 20908 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0117 19:16:38.583833 20908 solver.cpp:337] Iteration 1000, Testing net (#0)
I0117 19:16:38.681547 20908 solver.cpp:404]     Test net output #0: accuracy = 0.469
I0117 19:16:38.681547 20908 solver.cpp:404]     Test net output #1: loss = 1.37091 (* 1 = 1.37091 loss)
I0117 19:16:38.684551 20908 solver.cpp:228] Iteration 1000, loss = 1.30039
I0117 19:16:38.684551 20908 solver.cpp:244]     Train net output #0: loss = 1.30039 (* 1 = 1.30039 loss)
I0117 19:16:38.684551 20908 sgd_solver.cpp:106] Iteration 1000, lr = 0.00931012
I0117 19:16:49.543483 20908 solver.cpp:337] Iteration 2000, Testing net (#0)
I0117 19:16:49.642217 20908 solver.cpp:404]     Test net output #0: accuracy = 0.497
I0117 19:16:49.642217 20908 solver.cpp:404]     Test net output #1: loss = 1.40633 (* 1 = 1.40633 loss)
I0117 19:16:49.645210 20908 solver.cpp:228] Iteration 2000, loss = 1.00233
I0117 19:16:49.645210 20908 solver.cpp:244]     Train net output #0: loss = 1.00233 (* 1 = 1.00233 loss)
I0117 19:16:49.645210 20908 sgd_solver.cpp:106] Iteration 2000, lr = 0.00872196
I0117 19:17:00.499166 20908 solver.cpp:337] Iteration 3000, Testing net (#0)
I0117 19:17:00.598898 20908 solver.cpp:404]     Test net output #0: accuracy = 0.483
I0117 19:17:00.598898 20908 solver.cpp:404]     Test net output #1: loss = 1.81839 (* 1 = 1.81839 loss)
I0117 19:17:00.601889 20908 solver.cpp:228] Iteration 3000, loss = 0.53138
I0117 19:17:00.601889 20908 solver.cpp:244]     Train net output #0: loss = 0.53138 (* 1 = 0.53138 loss)
I0117 19:17:00.601889 20908 sgd_solver.cpp:106] Iteration 3000, lr = 0.00821377
I0117 19:17:11.444916 20908 solver.cpp:337] Iteration 4000, Testing net (#0)
I0117 19:17:11.542613 20908 solver.cpp:404]     Test net output #0: accuracy = 0.4865
I0117 19:17:11.542613 20908 solver.cpp:404]     Test net output #1: loss = 2.36197 (* 1 = 2.36197 loss)
I0117 19:17:11.545604 20908 solver.cpp:228] Iteration 4000, loss = 0.289861
I0117 19:17:11.545604 20908 solver.cpp:244]     Train net output #0: loss = 0.289861 (* 1 = 0.289861 loss)
I0117 19:17:11.545604 20908 sgd_solver.cpp:106] Iteration 4000, lr = 0.00776969
I0117 19:17:22.381608 20908 solver.cpp:337] Iteration 5000, Testing net (#0)
I0117 19:17:22.477382 20908 solver.cpp:404]     Test net output #0: accuracy = 0.484
I0117 19:17:22.478349 20908 solver.cpp:404]     Test net output #1: loss = 2.58122 (* 1 = 2.58122 loss)
I0117 19:17:22.481340 20908 solver.cpp:228] Iteration 5000, loss = 0.301774
I0117 19:17:22.481340 20908 solver.cpp:244]     Train net output #0: loss = 0.301774 (* 1 = 0.301774 loss)
I0117 19:17:22.481340 20908 sgd_solver.cpp:106] Iteration 5000, lr = 0.00737788
I0117 19:17:33.374192 20908 solver.cpp:337] Iteration 6000, Testing net (#0)
I0117 19:17:33.472928 20908 solver.cpp:404]     Test net output #0: accuracy = 0.497
I0117 19:17:33.472928 20908 solver.cpp:404]     Test net output #1: loss = 2.84809 (* 1 = 2.84809 loss)
I0117 19:17:33.475970 20908 solver.cpp:228] Iteration 6000, loss = 0.124637
I0117 19:17:33.475970 20908 solver.cpp:244]     Train net output #0: loss = 0.124637 (* 1 = 0.124637 loss)
I0117 19:17:33.475970 20908 sgd_solver.cpp:106] Iteration 6000, lr = 0.00702927
I0117 19:17:44.403677 20908 solver.cpp:337] Iteration 7000, Testing net (#0)
I0117 19:17:44.505404 20908 solver.cpp:404]     Test net output #0: accuracy = 0.526
I0117 19:17:44.505404 20908 solver.cpp:404]     Test net output #1: loss = 2.61905 (* 1 = 2.61905 loss)
I0117 19:17:44.508397 20908 solver.cpp:228] Iteration 7000, loss = 0.251587
I0117 19:17:44.508397 20908 solver.cpp:244]     Train net output #0: loss = 0.251587 (* 1 = 0.251587 loss)
I0117 19:17:44.508397 20908 sgd_solver.cpp:106] Iteration 7000, lr = 0.00671681
I0117 19:17:55.435156 20908 solver.cpp:337] Iteration 8000, Testing net (#0)
I0117 19:17:55.532927 20908 solver.cpp:404]     Test net output #0: accuracy = 0.5225
I0117 19:17:55.532927 20908 solver.cpp:404]     Test net output #1: loss = 2.68762 (* 1 = 2.68762 loss)
I0117 19:17:55.536885 20908 solver.cpp:228] Iteration 8000, loss = 0.00793565
I0117 19:17:55.536885 20908 solver.cpp:244]     Train net output #0: loss = 0.00793566 (* 1 = 0.00793566 loss)
I0117 19:17:55.536885 20908 sgd_solver.cpp:106] Iteration 8000, lr = 0.00643496
I0117 19:18:06.445693 20908 solver.cpp:337] Iteration 9000, Testing net (#0)
I0117 19:18:06.546423 20908 solver.cpp:404]     Test net output #0: accuracy = 0.5365
I0117 19:18:06.546423 20908 solver.cpp:404]     Test net output #1: loss = 2.60136 (* 1 = 2.60136 loss)
I0117 19:18:06.549415 20908 solver.cpp:228] Iteration 9000, loss = 0.037445
I0117 19:18:06.549415 20908 solver.cpp:244]     Train net output #0: loss = 0.037445 (* 1 = 0.037445 loss)
I0117 19:18:06.549415 20908 sgd_solver.cpp:106] Iteration 9000, lr = 0.00617924
I0117 19:18:17.459220 20908 solver.cpp:454] Snapshotting to binary proto file H:/1/caffe-windows/examples/face/model/_iter_10000.caffemodel
I0117 19:18:17.512130 20908 sgd_solver.cpp:273] Snapshotting solver state to binary proto file H:/1/caffe-windows/examples/face/model/_iter_10000.solverstate
I0117 19:18:17.645393 20908 solver.cpp:337] Iteration 10000, Testing net (#0)
I0117 19:18:17.739142 20908 solver.cpp:404]     Test net output #0: accuracy = 0.5245
I0117 19:18:17.739142 20908 solver.cpp:404]     Test net output #1: loss = 2.60895 (* 1 = 2.60895 loss)
I0117 19:18:17.742134 20908 solver.cpp:228] Iteration 10000, loss = 0.00650368
I0117 19:18:17.742134 20908 solver.cpp:244]     Train net output #0: loss = 0.00650359 (* 1 = 0.00650359 loss)
I0117 19:18:17.742134 20908 sgd_solver.cpp:106] Iteration 10000, lr = 0.00594604
I0117 19:18:28.684886 20908 solver.cpp:337] Iteration 11000, Testing net (#0)
I0117 19:18:28.783588 20908 solver.cpp:404]     Test net output #0: accuracy = 0.5325
I0117 19:18:28.783588 20908 solver.cpp:404]     Test net output #1: loss = 2.30051 (* 1 = 2.30051 loss)
I0117 19:18:28.786623 20908 solver.cpp:228] Iteration 11000, loss = 0.0696027
I0117 19:18:28.786623 20908 solver.cpp:244]     Train net output #0: loss = 0.0696025 (* 1 = 0.0696025 loss)
I0117 19:18:28.786623 20908 sgd_solver.cpp:106] Iteration 11000, lr = 0.00573239
I0117 19:18:39.715368 20908 solver.cpp:337] Iteration 12000, Testing net (#0)
I0117 19:18:39.814070 20908 solver.cpp:404]     Test net output #0: accuracy = 0.541
I0117 19:18:39.814070 20908 solver.cpp:404]     Test net output #1: loss = 2.32543 (* 1 = 2.32543 loss)
I0117 19:18:39.818060 20908 solver.cpp:228] Iteration 12000, loss = 0.00656059
I0117 19:18:39.818060 20908 solver.cpp:244]     Train net output #0: loss = 0.00656048 (* 1 = 0.00656048 loss)
I0117 19:18:39.818060 20908 sgd_solver.cpp:106] Iteration 12000, lr = 0.00553583
I0117 19:18:52.708565 20908 solver.cpp:337] Iteration 13000, Testing net (#0)
I0117 19:18:52.810292 20908 solver.cpp:404]     Test net output #0: accuracy = 0.528
I0117 19:18:52.810292 20908 solver.cpp:404]     Test net output #1: loss = 2.52416 (* 1 = 2.52416 loss)
I0117 19:18:52.813284 20908 solver.cpp:228] Iteration 13000, loss = 0.00818536
I0117 19:18:52.813284 20908 solver.cpp:244]     Train net output #0: loss = 0.00818531 (* 1 = 0.00818531 loss)
I0117 19:18:52.813284 20908 sgd_solver.cpp:106] Iteration 13000, lr = 0.00535432
I0117 19:19:08.506337 20908 solver.cpp:337] Iteration 14000, Testing net (#0)
I0117 19:19:08.634994 20908 solver.cpp:404]     Test net output #0: accuracy = 0.5285
I0117 19:19:08.634994 20908 solver.cpp:404]     Test net output #1: loss = 2.30122 (* 1 = 2.30122 loss)
I0117 19:19:08.639981 20908 solver.cpp:228] Iteration 14000, loss = 0.00692161
I0117 19:19:08.639981 20908 solver.cpp:244]     Train net output #0: loss = 0.0069216 (* 1 = 0.0069216 loss)
I0117 19:19:08.639981 20908 sgd_solver.cpp:106] Iteration 14000, lr = 0.00518611
I0117 19:19:22.940711 20908 solver.cpp:337] Iteration 15000, Testing net (#0)
I0117 19:19:23.044433 20908 solver.cpp:404]     Test net output #0: accuracy = 0.534
I0117 19:19:23.044433 20908 solver.cpp:404]     Test net output #1: loss = 2.35085 (* 1 = 2.35085 loss)
I0117 19:19:23.047425 20908 solver.cpp:228] Iteration 15000, loss = 0.00785264
I0117 19:19:23.047425 20908 solver.cpp:244]     Train net output #0: loss = 0.00785269 (* 1 = 0.00785269 loss)
I0117 19:19:23.048424 20908 sgd_solver.cpp:106] Iteration 15000, lr = 0.00502973
I0117 19:19:36.288034 20908 solver.cpp:337] Iteration 16000, Testing net (#0)
I0117 19:19:36.391716 20908 solver.cpp:404]     Test net output #0: accuracy = 0.531
I0117 19:19:36.391716 20908 solver.cpp:404]     Test net output #1: loss = 2.28968 (* 1 = 2.28968 loss)
I0117 19:19:36.395706 20908 solver.cpp:228] Iteration 16000, loss = 0.0066333
I0117 19:19:36.395706 20908 solver.cpp:244]     Train net output #0: loss = 0.00663341 (* 1 = 0.00663341 loss)
I0117 19:19:36.395706 20908 sgd_solver.cpp:106] Iteration 16000, lr = 0.00488394
I0117 19:19:50.890918 20908 solver.cpp:337] Iteration 17000, Testing net (#0)
I0117 19:19:50.996634 20908 solver.cpp:404]     Test net output #0: accuracy = 0.533
I0117 19:19:50.997632 20908 solver.cpp:404]     Test net output #1: loss = 2.37771 (* 1 = 2.37771 loss)
I0117 19:19:51.000624 20908 solver.cpp:228] Iteration 17000, loss = 0.0732821
I0117 19:19:51.000624 20908 solver.cpp:244]     Train net output #0: loss = 0.0732823 (* 1 = 0.0732823 loss)
I0117 19:19:51.000624 20908 sgd_solver.cpp:106] Iteration 17000, lr = 0.00474763
I0117 19:20:04.622206 20908 solver.cpp:337] Iteration 18000, Testing net (#0)
I0117 19:20:04.724898 20908 solver.cpp:404]     Test net output #0: accuracy = 0.525
I0117 19:20:04.724898 20908 solver.cpp:404]     Test net output #1: loss = 2.34104 (* 1 = 2.34104 loss)
I0117 19:20:04.727923 20908 solver.cpp:228] Iteration 18000, loss = 0.0106769
I0117 19:20:04.727923 20908 solver.cpp:244]     Train net output #0: loss = 0.010677 (* 1 = 0.010677 loss)
I0117 19:20:04.727923 20908 sgd_solver.cpp:106] Iteration 18000, lr = 0.00461989
I0117 19:20:18.189867 20908 solver.cpp:337] Iteration 19000, Testing net (#0)
I0117 19:20:18.297580 20908 solver.cpp:404]     Test net output #0: accuracy = 0.538
I0117 19:20:18.297580 20908 solver.cpp:404]     Test net output #1: loss = 2.18991 (* 1 = 2.18991 loss)
I0117 19:20:18.300571 20908 solver.cpp:228] Iteration 19000, loss = 0.00587837
I0117 19:20:18.300571 20908 solver.cpp:244]     Train net output #0: loss = 0.00587838 (* 1 = 0.00587838 loss)
I0117 19:20:18.300571 20908 sgd_solver.cpp:106] Iteration 19000, lr = 0.00449989
I0117 19:20:32.479626 20908 solver.cpp:454] Snapshotting to binary proto file H:/1/caffe-windows/examples/face/model/_iter_20000.caffemodel
I0117 19:20:32.547446 20908 sgd_solver.cpp:273] Snapshotting solver state to binary proto file H:/1/caffe-windows/examples/face/model/_iter_20000.solverstate
I0117 19:20:32.581355 20908 solver.cpp:317] Iteration 20000, loss = 0.00914669
I0117 19:20:32.582353 20908 solver.cpp:337] Iteration 20000, Testing net (#0)
I0117 19:20:32.681088 20908 solver.cpp:404]     Test net output #0: accuracy = 0.5385
I0117 19:20:32.681088 20908 solver.cpp:404]     Test net output #1: loss = 2.29346 (* 1 = 2.29346 loss)
I0117 19:20:32.681088 20908 solver.cpp:322] Optimization Done.
I0117 19:20:32.681088 20908 caffe.cpp:255] Optimization Done.
Epoch 0. Train Loss: 0.578158, Train Acc: 0.700269, Valid Loss: 1.423581, Valid Acc: 0.467361, Time 00:00:08
Epoch 1. Train Loss: 0.427627, Train Acc: 0.845430, Valid Loss: 0.750094, Valid Acc: 0.712500, Time 00:00:08
Epoch 2. Train Loss: 0.310827, Train Acc: 0.870968, Valid Loss: 0.190515, Valid Acc: 0.920139, Time 00:00:08
Epoch 3. Train Loss: 0.200619, Train Acc: 0.924731, Valid Loss: 0.349697, Valid Acc: 0.862500, Time 00:00:08
Epoch 4. Train Loss: 0.254690, Train Acc: 0.899194, Valid Loss: 0.351161, Valid Acc: 0.871528, Time 00:00:08
Epoch 5. Train Loss: 0.383361, Train Acc: 0.870968, Valid Loss: 0.211257, Valid Acc: 0.927778, Time 00:00:08
Epoch 6. Train Loss: 0.117126, Train Acc: 0.955645, Valid Loss: 0.158174, Valid Acc: 0.962500, Time 00:00:08
Epoch 7. Train Loss: 0.280084, Train Acc: 0.892473, Valid Loss: 0.784631, Valid Acc: 0.768056, Time 00:00:08
Epoch 8. Train Loss: 0.251075, Train Acc: 0.920699, Valid Loss: 0.127921, Valid Acc: 0.943750, Time 00:00:08
Epoch 9. Train Loss: 0.140907, Train Acc: 0.951613, Valid Loss: 0.165842, Valid Acc: 0.950000, Time 00:00:08
Epoch 10. Train Loss: 0.165661, Train Acc: 0.947581, Valid Loss: 0.253388, Valid Acc: 0.913889, Time 00:00:08
Epoch 0. Train Loss: 0.465703, Train Acc: 0.775538, Valid Loss: 0.356831, Valid Acc: 0.830556, Time 00:00:07
Epoch 1. Train Loss: 0.277387, Train Acc: 0.904570, Valid Loss: 0.745054, Valid Acc: 0.700000, Time 00:00:08
Epoch 2. Train Loss: 0.230781, Train Acc: 0.895161, Valid Loss: 0.135576, Valid Acc: 0.938889, Time 00:00:08
Epoch 3. Train Loss: 0.327682, Train Acc: 0.850806, Valid Loss: 0.321438, Valid Acc: 0.896528, Time 00:00:08
Epoch 4. Train Loss: 0.194975, Train Acc: 0.903226, Valid Loss: 0.212700, Valid Acc: 0.920139, Time 00:00:08
Epoch 5. Train Loss: 0.212196, Train Acc: 0.923387, Valid Loss: 0.624637, Valid Acc: 0.805556, Time 00:00:08
Epoch 6. Train Loss: 0.243527, Train Acc: 0.911290, Valid Loss: 0.253253, Valid Acc: 0.920139, Time 00:00:08
Epoch 7. Train Loss: 0.192588, Train Acc: 0.919355, Valid Loss: 0.262576, Valid Acc: 0.901389, Time 00:00:08
Epoch 8. Train Loss: 0.100727, Train Acc: 0.959677, Valid Loss: 0.235709, Valid Acc: 0.938889, Time 00:00:08
Epoch 9. Train Loss: 0.228661, Train Acc: 0.912634, Valid Loss: 0.239035, Valid Acc: 0.913889, Time 00:00:08
Epoch 10. Train Loss: 0.223252, Train Acc: 0.911290, Valid Loss: 0.214346, Valid Acc: 0.921528, Time 00:00:08
Epoch 0. Train Loss: 0.560907, Train Acc: 0.694892, Valid Loss: 0.884760, Valid Acc: 0.637500, Time 00:00:07
Epoch 1. Train Loss: 0.272352, Train Acc: 0.891129, Valid Loss: 0.133777, Valid Acc: 0.951389, Time 00:00:08
Epoch 2. Train Loss: 0.463632, Train Acc: 0.815860, Valid Loss: 0.402060, Valid Acc: 0.850000, Time 00:00:08
Epoch 3. Train Loss: 0.233415, Train Acc: 0.900538, Valid Loss: 0.160360, Valid Acc: 0.938889, Time 00:00:08
Epoch 4. Train Loss: 0.217820, Train Acc: 0.899194, Valid Loss: 0.159597, Valid Acc: 0.938889, Time 00:00:08
Epoch 5. Train Loss: 0.198051, Train Acc: 0.891129, Valid Loss: 0.165299, Valid Acc: 0.934028, Time 00:00:08
Epoch 6. Train Loss: 0.110266, Train Acc: 0.938172, Valid Loss: 0.219164, Valid Acc: 0.943750, Time 00:00:08
Epoch 7. Train Loss: 0.136834, Train Acc: 0.955645, Valid Loss: 0.135120, Valid Acc: 0.940278, Time 00:00:08
Epoch 8. Train Loss: 0.247052, Train Acc: 0.923387, Valid Loss: 0.155331, Valid Acc: 0.962500, Time 00:00:08
Epoch 9. Train Loss: 0.156940, Train Acc: 0.939516, Valid Loss: 0.182224, Valid Acc: 0.932639, Time 00:00:08
Epoch 10. Train Loss: 0.106106, Train Acc: 0.947581, Valid Loss: 0.221770, Valid Acc: 0.926389, Time 00:00:08
Epoch 11. Train Loss: 0.221326, Train Acc: 0.919355, Valid Loss: 0.212194, Valid Acc: 0.932639, Time 00:00:09
Epoch 12. Train Loss: 0.292550, Train Acc: 0.901882, Valid Loss: 0.301076, Valid Acc: 0.906250, Time 00:00:08
Epoch 13. Train Loss: 0.248876, Train Acc: 0.919355, Valid Loss: 0.225559, Valid Acc: 0.938889, Time 00:00:08
Epoch 14. Train Loss: 0.181533, Train Acc: 0.936828, Valid Loss: 2.623576, Valid Acc: 0.643750, Time 00:00:08
Epoch 15. Train Loss: 0.130769, Train Acc: 0.940860, Valid Loss: 0.448226, Valid Acc: 0.895139, Time 00:00:08
Epoch 16. Train Loss: 0.305345, Train Acc: 0.884409, Valid Loss: 0.336636, Valid Acc: 0.925000, Time 00:00:08
Epoch 17. Train Loss: 0.159392, Train Acc: 0.928763, Valid Loss: 0.473858, Valid Acc: 0.882639, Time 00:00:08
Epoch 18. Train Loss: 0.205520, Train Acc: 0.923387, Valid Loss: 0.222976, Valid Acc: 0.957639, Time 00:00:08
Epoch 19. Train Loss: 0.145989, Train Acc: 0.951613, Valid Loss: 0.214137, Valid Acc: 0.945139, Time 00:00:08
Epoch 0. Train Loss: 0.587014, Train Acc: 0.684140, Valid Loss: 0.364062, Valid Acc: 0.870139, Time 00:00:07
Epoch 1. Train Loss: 0.275530, Train Acc: 0.892473, Valid Loss: 0.185482, Valid Acc: 0.938889, Time 00:00:08
Epoch 2. Train Loss: 0.267221, Train Acc: 0.889785, Valid Loss: 1.389251, Valid Acc: 0.603472, Time 00:00:08
Epoch 3. Train Loss: 0.197974, Train Acc: 0.927419, Valid Loss: 0.195061, Valid Acc: 0.926389, Time 00:00:08
Epoch 4. Train Loss: 0.270485, Train Acc: 0.915323, Valid Loss: 0.237358, Valid Acc: 0.920139, Time 00:00:08
Epoch 5. Train Loss: 0.278512, Train Acc: 0.931452, Valid Loss: 0.172622, Valid Acc: 0.957639, Time 00:00:08
Epoch 6. Train Loss: 0.245009, Train Acc: 0.893817, Valid Loss: 0.374971, Valid Acc: 0.882639, Time 00:00:08
Epoch 7. Train Loss: 0.176684, Train Acc: 0.943548, Valid Loss: 0.180959, Valid Acc: 0.951389, Time 00:00:08
Epoch 8. Train Loss: 0.212765, Train Acc: 0.927419, Valid Loss: 0.222929, Valid Acc: 0.920139, Time 00:00:08
Epoch 9. Train Loss: 0.130577, Train Acc: 0.943548, Valid Loss: 0.258850, Valid Acc: 0.945139, Time 00:00:08
Epoch 10. Train Loss: 0.189150, Train Acc: 0.935484, Valid Loss: 0.303583, Valid Acc: 0.920139, Time 00:00:08
Epoch 11. Train Loss: 0.255441, Train Acc: 0.915323, Valid Loss: 0.330318, Valid Acc: 0.920139, Time 00:00:09
Epoch 12. Train Loss: 0.163954, Train Acc: 0.916667, Valid Loss: 1.028040, Valid Acc: 0.743056, Time 00:00:08
Epoch 13. Train Loss: 0.204388, Train Acc: 0.923387, Valid Loss: 0.316657, Valid Acc: 0.888889, Time 00:00:08
Epoch 14. Train Loss: 0.171159, Train Acc: 0.943548, Valid Loss: 0.280547, Valid Acc: 0.920139, Time 00:00:08
Epoch 15. Train Loss: 0.215625, Train Acc: 0.911290, Valid Loss: 0.453189, Valid Acc: 0.876389, Time 00:00:08
Epoch 16. Train Loss: 0.196871, Train Acc: 0.926075, Valid Loss: 0.689498, Valid Acc: 0.845139, Time 00:00:08
Epoch 17. Train Loss: 0.226891, Train Acc: 0.911290, Valid Loss: 0.241059, Valid Acc: 0.913889, Time 00:00:08
Epoch 18. Train Loss: 0.206625, Train Acc: 0.923387, Valid Loss: 0.238922, Valid Acc: 0.907639, Time 00:00:08
Epoch 19. Train Loss: 0.056393, Train Acc: 0.987903, Valid Loss: 0.226941, Valid Acc: 0.945139, Time 00:00:08
Epoch 20. Train Loss: 0.134435, Train Acc: 0.958333, Valid Loss: 0.430172, Valid Acc: 0.865278, Time 00:00:08
Epoch 21. Train Loss: 0.142814, Train Acc: 0.920699, Valid Loss: 1.115362, Valid Acc: 0.779167, Time 00:00:08
Epoch 22. Train Loss: 0.229004, Train Acc: 0.931452, Valid Loss: 0.562894, Valid Acc: 0.862500, Time 00:00:08
Epoch 23. Train Loss: 0.074949, Train Acc: 0.967742, Valid Loss: 0.376665, Valid Acc: 0.920139, Time 00:00:08
Epoch 24. Train Loss: 0.092488, Train Acc: 0.963710, Valid Loss: 0.254518, Valid Acc: 0.920139, Time 00:00:08
Epoch 25. Train Loss: 0.197432, Train Acc: 0.923387, Valid Loss: 0.435778, Valid Acc: 0.888889, Time 00:00:08
Epoch 26. Train Loss: 0.176928, Train Acc: 0.931452, Valid Loss: 0.259856, Valid Acc: 0.932639, Time 00:00:08
Epoch 27. Train Loss: 0.075768, Train Acc: 0.967742, Valid Loss: 0.301955, Valid Acc: 0.926389, Time 00:00:08
Epoch 28. Train Loss: 0.127211, Train Acc: 0.959677, Valid Loss: 0.286939, Valid Acc: 0.907639, Time 00:00:08
Epoch 29. Train Loss: 0.180415, Train Acc: 0.932796, Valid Loss: 0.277954, Valid Acc: 0.926389, Time 00:00:08
Epoch 30. Train Loss: 0.159525, Train Acc: 0.947581, Valid Loss: 0.344457, Valid Acc: 0.890278, Time 00:00:08
Epoch 31. Train Loss: 0.156262, Train Acc: 0.948925, Valid Loss: 0.338127, Valid Acc: 0.895139, Time 00:00:08
Epoch 32. Train Loss: 0.145002, Train Acc: 0.951613, Valid Loss: 0.290621, Valid Acc: 0.920139, Time 00:00:08
Epoch 33. Train Loss: 0.162693, Train Acc: 0.934140, Valid Loss: 0.677105, Valid Acc: 0.777778, Time 00:00:08
Epoch 34. Train Loss: 0.121158, Train Acc: 0.961022, Valid Loss: 0.826174, Valid Acc: 0.766667, Time 00:00:08
Epoch 35. Train Loss: 0.265449, Train Acc: 0.919355, Valid Loss: 0.311649, Valid Acc: 0.926389, Time 00:00:08
Epoch 36. Train Loss: 0.109944, Train Acc: 0.955645, Valid Loss: 0.415549, Valid Acc: 0.907639, Time 00:00:08
Epoch 37. Train Loss: 0.214035, Train Acc: 0.939516, Valid Loss: 0.369407, Valid Acc: 0.926389, Time 00:00:08
Epoch 38. Train Loss: 0.070020, Train Acc: 0.975806, Valid Loss: 0.395070, Valid Acc: 0.913889, Time 00:00:08
Epoch 39. Train Loss: 0.133804, Train Acc: 0.934140, Valid Loss: 0.952799, Valid Acc: 0.749306, Time 00:00:08
Epoch 0. Train Loss: 0.726555, Train Acc: 0.705645, Valid Loss: 0.233635, Valid Acc: 0.891667, Time 00:00:07
Epoch 1. Train Loss: 0.289083, Train Acc: 0.899194, Valid Loss: 0.167052, Valid Acc: 0.956250, Time 00:00:08
Epoch 2. Train Loss: 0.293700, Train Acc: 0.870968, Valid Loss: 0.150479, Valid Acc: 0.945139, Time 00:00:08
Epoch 3. Train Loss: 0.305803, Train Acc: 0.891129, Valid Loss: 0.169401, Valid Acc: 0.938889, Time 00:00:08
Epoch 4. Train Loss: 0.302927, Train Acc: 0.880376, Valid Loss: 0.301308, Valid Acc: 0.888889, Time 00:00:08
Epoch 5. Train Loss: 0.198019, Train Acc: 0.931452, Valid Loss: 0.444149, Valid Acc: 0.868750, Time 00:00:08
Epoch 6. Train Loss: 0.146529, Train Acc: 0.935484, Valid Loss: 0.286849, Valid Acc: 0.897917, Time 00:00:08
Epoch 7. Train Loss: 0.103370, Train Acc: 0.951613, Valid Loss: 0.190367, Valid Acc: 0.927778, Time 00:00:08
Epoch 8. Train Loss: 0.097430, Train Acc: 0.967742, Valid Loss: 0.412981, Valid Acc: 0.824306, Time 00:00:08
Epoch 9. Train Loss: 0.115004, Train Acc: 0.956989, Valid Loss: 0.186642, Valid Acc: 0.920139, Time 00:00:08
Epoch 10. Train Loss: 0.166791, Train Acc: 0.926075, Valid Loss: 0.475968, Valid Acc: 0.866667, Time 00:00:08
Epoch 11. Train Loss: 0.154252, Train Acc: 0.939516, Valid Loss: 0.228025, Valid Acc: 0.915278, Time 00:00:09
Epoch 12. Train Loss: 0.119292, Train Acc: 0.959677, Valid Loss: 0.291780, Valid Acc: 0.902778, Time 00:00:08
Epoch 13. Train Loss: 0.104478, Train Acc: 0.971774, Valid Loss: 0.365712, Valid Acc: 0.872917, Time 00:00:08
Epoch 14. Train Loss: 0.024904, Train Acc: 0.995968, Valid Loss: 0.218505, Valid Acc: 0.921528, Time 00:00:08
Epoch 15. Train Loss: 0.043324, Train Acc: 0.979839, Valid Loss: 0.268435, Valid Acc: 0.895139, Time 00:00:08
Epoch 16. Train Loss: 0.144514, Train Acc: 0.950269, Valid Loss: 0.865374, Valid Acc: 0.800000, Time 00:00:08
Epoch 17. Train Loss: 0.106935, Train Acc: 0.955645, Valid Loss: 0.551937, Valid Acc: 0.805556, Time 00:00:08
Epoch 18. Train Loss: 0.097883, Train Acc: 0.967742, Valid Loss: 0.233580, Valid Acc: 0.921528, Time 00:00:08
Epoch 19. Train Loss: 0.033220, Train Acc: 0.985215, Valid Loss: 0.311862, Valid Acc: 0.918750, Time 00:00:08
Epoch 20. Train Loss: 0.077877, Train Acc: 0.969086, Valid Loss: 0.333705, Valid Acc: 0.891667, Time 00:00:08
Epoch 21. Train Loss: 0.187425, Train Acc: 0.922043, Valid Loss: 1.270235, Valid Acc: 0.743750, Time 00:00:08
Epoch 22. Train Loss: 0.158028, Train Acc: 0.935484, Valid Loss: 1.054781, Valid Acc: 0.747917, Time 00:00:09
Epoch 23. Train Loss: 0.139946, Train Acc: 0.947581, Valid Loss: 0.200665, Valid Acc: 0.931250, Time 00:00:08
Epoch 24. Train Loss: 0.109903, Train Acc: 0.971774, Valid Loss: 0.200405, Valid Acc: 0.913889, Time 00:00:08
Epoch 25. Train Loss: 0.142085, Train Acc: 0.962366, Valid Loss: 0.521589, Valid Acc: 0.868750, Time 00:00:08
Epoch 26. Train Loss: 0.083613, Train Acc: 0.971774, Valid Loss: 0.236238, Valid Acc: 0.884028, Time 00:00:08
Epoch 27. Train Loss: 0.211441, Train Acc: 0.952957, Valid Loss: 0.421707, Valid Acc: 0.868750, Time 00:00:08
Epoch 28. Train Loss: 0.038109, Train Acc: 0.983871, Valid Loss: 0.261481, Valid Acc: 0.913889, Time 00:00:08
Epoch 29. Train Loss: 0.054774, Train Acc: 0.979839, Valid Loss: 0.205149, Valid Acc: 0.931250, Time 00:00:08
Epoch 30. Train Loss: 0.062292, Train Acc: 0.983871, Valid Loss: 0.282510, Valid Acc: 0.943750, Time 00:00:08
Epoch 31. Train Loss: 0.108582, Train Acc: 0.967742, Valid Loss: 0.257158, Valid Acc: 0.910417, Time 00:00:08
Epoch 32. Train Loss: 0.022149, Train Acc: 0.991935, Valid Loss: 0.211315, Valid Acc: 0.927778, Time 00:00:08
Epoch 33. Train Loss: 0.094659, Train Acc: 0.971774, Valid Loss: 0.433674, Valid Acc: 0.860417, Time 00:00:09
Epoch 34. Train Loss: 0.067796, Train Acc: 0.985215, Valid Loss: 2.669041, Valid Acc: 0.536111, Time 00:00:08
Epoch 35. Train Loss: 0.149794, Train Acc: 0.942204, Valid Loss: 0.682437, Valid Acc: 0.804167, Time 00:00:08
Epoch 36. Train Loss: 0.077451, Train Acc: 0.975806, Valid Loss: 0.186498, Valid Acc: 0.945139, Time 00:00:08
Epoch 37. Train Loss: 0.036308, Train Acc: 0.991935, Valid Loss: 0.204960, Valid Acc: 0.915278, Time 00:00:08
Epoch 38. Train Loss: 0.018603, Train Acc: 0.995968, Valid Loss: 0.238901, Valid Acc: 0.915278, Time 00:00:08
Epoch 39. Train Loss: 0.045968, Train Acc: 0.983871, Valid Loss: 0.363907, Valid Acc: 0.866667, Time 00:00:08
Epoch 0. Train Loss: 0.635680, Train Acc: 0.672043, Valid Loss: 1.664328, Valid Acc: 0.568750, Time 00:00:07
Epoch 1. Train Loss: 0.327201, Train Acc: 0.862903, Valid Loss: 0.264857, Valid Acc: 0.909028, Time 00:00:08
Epoch 2. Train Loss: 0.192154, Train Acc: 0.939516, Valid Loss: 0.157425, Valid Acc: 0.926389, Time 00:00:08
Epoch 3. Train Loss: 0.247584, Train Acc: 0.873656, Valid Loss: 1.806764, Valid Acc: 0.542361, Time 00:00:08
Epoch 4. Train Loss: 0.257309, Train Acc: 0.908602, Valid Loss: 1.766576, Valid Acc: 0.687500, Time 00:00:08
Epoch 5. Train Loss: 0.314668, Train Acc: 0.897849, Valid Loss: 0.296355, Valid Acc: 0.877778, Time 00:00:08
Epoch 6. Train Loss: 0.205931, Train Acc: 0.907258, Valid Loss: 0.249086, Valid Acc: 0.902778, Time 00:00:08
Epoch 7. Train Loss: 0.219435, Train Acc: 0.935484, Valid Loss: 0.184470, Valid Acc: 0.940278, Time 00:00:08
Epoch 8. Train Loss: 0.123030, Train Acc: 0.955645, Valid Loss: 0.174388, Valid Acc: 0.938889, Time 00:00:08
Epoch 9. Train Loss: 0.095044, Train Acc: 0.954301, Valid Loss: 0.567411, Valid Acc: 0.843750, Time 00:00:08
Epoch 10. Train Loss: 0.142099, Train Acc: 0.938172, Valid Loss: 0.203809, Valid Acc: 0.950000, Time 00:00:08
Epoch 0. Train Loss: 0.610881, Train Acc: 0.700269, Valid Loss: 1.406981, Valid Acc: 0.461111, Time 00:00:08
Epoch 1. Train Loss: 0.311663, Train Acc: 0.858871, Valid Loss: 0.169640, Valid Acc: 0.943750, Time 00:00:08
Epoch 2. Train Loss: 0.186758, Train Acc: 0.927419, Valid Loss: 0.196251, Valid Acc: 0.921528, Time 00:00:08
Epoch 3. Train Loss: 0.191882, Train Acc: 0.943548, Valid Loss: 0.148043, Valid Acc: 0.934028, Time 00:00:08
Epoch 4. Train Loss: 0.236756, Train Acc: 0.899194, Valid Loss: 0.157217, Valid Acc: 0.943750, Time 00:00:08
Epoch 5. Train Loss: 0.209826, Train Acc: 0.920699, Valid Loss: 0.421972, Valid Acc: 0.862500, Time 00:00:08
Epoch 6. Train Loss: 0.200140, Train Acc: 0.939516, Valid Loss: 0.357544, Valid Acc: 0.918750, Time 00:00:08
Epoch 7. Train Loss: 0.160918, Train Acc: 0.935484, Valid Loss: 0.326889, Valid Acc: 0.907639, Time 00:00:08
Epoch 8. Train Loss: 0.201199, Train Acc: 0.920699, Valid Loss: 0.961639, Valid Acc: 0.711806, Time 00:00:08
Epoch 9. Train Loss: 0.212855, Train Acc: 0.889785, Valid Loss: 0.352098, Valid Acc: 0.871528, Time 00:00:08
Epoch 10. Train Loss: 0.098841, Train Acc: 0.967742, Valid Loss: 0.189939, Valid Acc: 0.932639, Time 00:00:08
Epoch 11. Train Loss: 0.112414, Train Acc: 0.950269, Valid Loss: 0.721353, Valid Acc: 0.822917, Time 00:00:09
Epoch 0. Train Loss: 0.525856, Train Acc: 0.721774, Valid Loss: 0.192599, Valid Acc: 0.931250, Time 00:00:09
Epoch 1. Train Loss: 0.414308, Train Acc: 0.819892, Valid Loss: 0.901098, Valid Acc: 0.639583, Time 00:00:09
Epoch 2. Train Loss: 0.197426, Train Acc: 0.923387, Valid Loss: 0.130372, Valid Acc: 0.934028, Time 00:00:09
Epoch 3. Train Loss: 0.209236, Train Acc: 0.935484, Valid Loss: 0.112546, Valid Acc: 0.950000, Time 00:00:09
Epoch 4. Train Loss: 0.209017, Train Acc: 0.915323, Valid Loss: 0.192004, Valid Acc: 0.921528, Time 00:00:09
Epoch 5. Train Loss: 0.140516, Train Acc: 0.959677, Valid Loss: 0.179908, Valid Acc: 0.932639, Time 00:00:09
Epoch 6. Train Loss: 0.141939, Train Acc: 0.947581, Valid Loss: 0.130691, Valid Acc: 0.943750, Time 00:00:09
Epoch 7. Train Loss: 0.158480, Train Acc: 0.946237, Valid Loss: 1.268154, Valid Acc: 0.668750, Time 00:00:09
Epoch 8. Train Loss: 0.110208, Train Acc: 0.951613, Valid Loss: 0.254351, Valid Acc: 0.915278, Time 00:00:09
Epoch 9. Train Loss: 0.155293, Train Acc: 0.936828, Valid Loss: 0.449161, Valid Acc: 0.812500, Time 00:00:09
Epoch 10. Train Loss: 0.070769, Train Acc: 0.967742, Valid Loss: 0.326566, Valid Acc: 0.920139, Time 00:00:09
Epoch 11. Train Loss: 0.109998, Train Acc: 0.959677, Valid Loss: 0.368834, Valid Acc: 0.885417, Time 00:00:10
Epoch 12. Train Loss: 0.133513, Train Acc: 0.955645, Valid Loss: 0.193978, Valid Acc: 0.921528, Time 00:00:09
Epoch 13. Train Loss: 0.084593, Train Acc: 0.950269, Valid Loss: 0.509030, Valid Acc: 0.863889, Time 00:00:09
Epoch 14. Train Loss: 0.181070, Train Acc: 0.935484, Valid Loss: 0.356245, Valid Acc: 0.902778, Time 00:00:09
Epoch 15. Train Loss: 0.108668, Train Acc: 0.963710, Valid Loss: 0.270529, Valid Acc: 0.909028, Time 00:00:09
Epoch 16. Train Loss: 0.109722, Train Acc: 0.951613, Valid Loss: 0.413112, Valid Acc: 0.866667, Time 00:00:09
Epoch 0. Train Loss: 0.628532, Train Acc: 0.662634, Valid Loss: 0.629187, Valid Acc: 0.737500, Time 00:00:09
Epoch 1. Train Loss: 0.339771, Train Acc: 0.866935, Valid Loss: 0.167214, Valid Acc: 0.922917, Time 00:00:09
Epoch 2. Train Loss: 0.181558, Train Acc: 0.919355, Valid Loss: 0.146556, Valid Acc: 0.951389, Time 00:00:08
Epoch 3. Train Loss: 0.153845, Train Acc: 0.935484, Valid Loss: 0.178033, Valid Acc: 0.945139, Time 00:00:08
Epoch 4. Train Loss: 0.137454, Train Acc: 0.939516, Valid Loss: 0.152688, Valid Acc: 0.922917, Time 00:00:08
Epoch 5. Train Loss: 0.106211, Train Acc: 0.955645, Valid Loss: 0.274384, Valid Acc: 0.902778, Time 00:00:08
Epoch 0. Train Loss: 0.671181, Train Acc: 0.668011, Valid Loss: 1.078775, Valid Acc: 0.606250, Time 00:00:07
Epoch 1. Train Loss: 0.364361, Train Acc: 0.852151, Valid Loss: 0.166185, Valid Acc: 0.956250, Time 00:00:08
Epoch 2. Train Loss: 0.172665, Train Acc: 0.911290, Valid Loss: 0.144650, Valid Acc: 0.916667, Time 00:00:08
